{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1_/f68ylrks4638ccdltlx2l91h0000gn/T/ipykernel_68379/1969592306.py:87: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'alphabet' must be None, 'ordinal' or array-like with shape (n_bins,) (got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 749\u001b[0m\n\u001b[1;32m    746\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_06_08.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;66;03m# Load full data and fit scaler with the first training set\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m full_data, obs_data, unscaled_data, scaler, sax_scaler \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;66;03m# Ensure 'timestamp' is in the dataframe\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m full_data\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "Cell \u001b[0;32mIn[4], line 179\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[0;34m(filepath, scaler, fit_scaler, denoise_method, window_size, data, month_train, sax_window_size, sax_word_size, sax_alphabet_size)\u001b[0m\n\u001b[1;32m    176\u001b[0m df_obs \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Calculate SAX features on unscaled 'Close' price\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m sax_features \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_sax_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_unscaled\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msax_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msax_word_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43malphabet_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msax_alphabet_size\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Normalize SAX features\u001b[39;00m\n\u001b[1;32m    187\u001b[0m sax_scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n",
      "Cell \u001b[0;32mIn[4], line 72\u001b[0m, in \u001b[0;36mcompute_sax_features\u001b[0;34m(series, window_size, word_size, alphabet_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m window \u001b[38;5;241m=\u001b[39m series\u001b[38;5;241m.\u001b[39miloc[i \u001b[38;5;241m-\u001b[39m window_size:i]\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Transform the window using SAX\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m window_transformed \u001b[38;5;241m=\u001b[39m \u001b[43msax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Calculate frequency of each symbol in the transformed window\u001b[39;00m\n\u001b[1;32m     75\u001b[0m unique, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(window_transformed, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/trading_monitor/trading_env/lib/python3.11/site-packages/pyts/approximation/sax.py:93\u001b[0m, in \u001b[0;36mSymbolicAggregateApproximation.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     91\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(X, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     92\u001b[0m n_timestamps \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 93\u001b[0m alphabet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_timestamps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m discretizer \u001b[38;5;241m=\u001b[39m KBinsDiscretizer(\n\u001b[1;32m     95\u001b[0m     n_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_bins, strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy,\n\u001b[1;32m     96\u001b[0m     raise_warning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraise_warning\n\u001b[1;32m     97\u001b[0m )\n\u001b[1;32m     98\u001b[0m indices \u001b[38;5;241m=\u001b[39m discretizer\u001b[38;5;241m.\u001b[39mfit_transform(X)\n",
      "File \u001b[0;32m~/trading_monitor/trading_env/lib/python3.11/site-packages/pyts/approximation/sax.py:119\u001b[0m, in \u001b[0;36mSymbolicAggregateApproximation._check_params\u001b[0;34m(self, n_timestamps)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrategy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be either \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantile\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphabet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphabet \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mordinal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphabet, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray)))):\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malphabet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be None, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mordinal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or array-like \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith shape (n_bins,) (got \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphabet))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphabet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     alphabet \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mchr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m97\u001b[39m, \u001b[38;5;241m97\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_bins)])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'alphabet' must be None, 'ordinal' or array-like with shape (n_bins,) (got 4)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch as th\n",
    "import joblib\n",
    "from scipy.signal import argrelextrema\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Custom Neural Network Architecture\n",
    "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box):\n",
    "        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim=256)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_input_channels, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.net(observations)\n",
    "\n",
    "def find_local_extrema(prices, order=5):\n",
    "    prices = np.array(prices)\n",
    "    minima_indices = argrelextrema(prices, np.less_equal, order=order)[0]\n",
    "    maxima_indices = argrelextrema(prices, np.greater_equal, order=order)[0]\n",
    "    return minima_indices, maxima_indices\n",
    "\n",
    "def preprocess_data(filepath=None, scaler=None, fit_scaler=False, denoise_method='moving_average', window_size=5, data=None, month_train=None):\n",
    "    if data is None:\n",
    "        df = pd.read_csv(\n",
    "            filepath,\n",
    "            parse_dates=['timestamp'],\n",
    "            date_parser=lambda col: pd.to_datetime(col, utc=True)\n",
    "        )\n",
    "    else:\n",
    "        df = data.copy()\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    df = df.set_index('timestamp')\n",
    "    \n",
    "    # Check for column names and select accordingly\n",
    "    if all(col in df.columns for col in ['open', 'high', 'low', 'close', 'volume']):\n",
    "        df = df[['open', 'high', 'low', 'close', 'volume']]\n",
    "        df = df.rename(\n",
    "            columns={\n",
    "                'open': 'Open',\n",
    "                'high': 'High',\n",
    "                'low': 'Low',\n",
    "                'close': 'Close',\n",
    "                'volume': 'Volume'\n",
    "            }\n",
    "        )\n",
    "    elif all(col in df.columns for col in ['Open', 'High', 'Low', 'Close', 'Volume']):\n",
    "        df = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "    else:\n",
    "        raise KeyError(\"Dataframe does not have the required columns.\")\n",
    "    \n",
    "    # Remove rows with zero or negative prices\n",
    "    df = df[df['Close'] > 0]\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Denoise data\n",
    "    if denoise_method == 'moving_average':\n",
    "        df['Close'] = df['Close'].rolling(window=window_size, min_periods=1).mean()\n",
    "        df['Open'] = df['Open'].rolling(window=window_size, min_periods=1).mean()\n",
    "        df['High'] = df['High'].rolling(window=window_size, min_periods=1).mean()\n",
    "        df['Low'] = df['Low'].rolling(window=window_size, min_periods=1).mean()\n",
    "    elif denoise_method == 'butterworth':\n",
    "        # Apply Butterworth filter\n",
    "        from scipy.signal import butter, filtfilt\n",
    "        def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "            nyq = 0.5 * fs\n",
    "            normal_cutoff = cutoff / nyq\n",
    "            b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "            y = filtfilt(b, a, data)\n",
    "            return y\n",
    "        cutoff = 0.1  # Adjust as needed\n",
    "        fs = 1        # Sampling frequency\n",
    "        for col in ['Open', 'High', 'Low', 'Close']:\n",
    "            df[col] = butter_lowpass_filter(df[col], cutoff, fs)\n",
    "    elif denoise_method == 'wavelet':\n",
    "        # Apply Wavelet Denoising\n",
    "        import pywt\n",
    "        def wavelet_denoise(data):\n",
    "            coeffs = pywt.wavedec(data, 'db1', level=2)\n",
    "            coeffs[1:] = [pywt.threshold(i, value=0.1 * max(i)) for i in coeffs[1:]]\n",
    "            return pywt.waverec(coeffs, 'db1')[:len(data)]\n",
    "        for col in ['Open', 'High', 'Low', 'Close']:\n",
    "            df[col] = wavelet_denoise(df[col].values)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported denoise method: {denoise_method}\")\n",
    "\n",
    "    # Keep a copy of the unscaled data for plotting and indicator calculation\n",
    "    df_unscaled = df.copy().reset_index()\n",
    "\n",
    "    # Normalize input features (except 'Close' price)\n",
    "    if scaler is None and fit_scaler:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df[['Open', 'High', 'Low', 'Volume']])\n",
    "        if month_train is not None:\n",
    "            scaler_filename = f'scaler_{month_train}.save'\n",
    "        else:\n",
    "            scaler_filename = 'scaler.save'\n",
    "        joblib.dump(scaler, scaler_filename)\n",
    "    elif scaler is None:\n",
    "        if month_train is not None:\n",
    "            scaler_filename = f'scaler_{month_train}.save'\n",
    "        else:\n",
    "            scaler_filename = 'scaler.save'\n",
    "        scaler = joblib.load(scaler_filename)\n",
    "\n",
    "    df[['Open', 'High', 'Low', 'Volume']] = scaler.transform(\n",
    "        df[['Open', 'High', 'Low', 'Volume']]\n",
    "    )\n",
    "\n",
    "    # Create df_full with all columns and reset index to include 'timestamp'\n",
    "    df_full = df.copy().reset_index()\n",
    "\n",
    "    # Create df_obs without 'Open', 'High', 'Low', and without 'timestamp'\n",
    "    df_obs = df[['Close', 'Volume']].copy().reset_index(drop=True)\n",
    "\n",
    "    # Return unscaled data as well\n",
    "    return df_full, df_obs.values, df_unscaled, scaler\n",
    "\n",
    "def detect_price_pattern(prices, window=20):\n",
    "    returns = np.diff(prices) / prices[:-1]\n",
    "    rolling_mean = np.convolve(returns, np.ones(window), 'valid') / window\n",
    "\n",
    "    # Calculate the slope of the trend line\n",
    "    x = np.arange(len(rolling_mean))\n",
    "    slope, _, _, _, _ = stats.linregress(x, rolling_mean)\n",
    "\n",
    "    # Determine the pattern based on the slope and its statistical significance\n",
    "    t_stat = slope / (np.std(rolling_mean) / np.sqrt(len(rolling_mean)))\n",
    "\n",
    "    if t_stat > 2:  # Statistically significant positive trend\n",
    "        return \"Uptrend\"\n",
    "    elif t_stat < -2:  # Statistically significant negative trend\n",
    "        return \"Downtrend\"\n",
    "    else:\n",
    "        return \"Sideways\"\n",
    "\n",
    "# Indicator functions\n",
    "def ema(series, period):\n",
    "    return series.ewm(span=period, adjust=False).mean()\n",
    "\n",
    "def rsi(series, period=14):\n",
    "    delta = series.diff(1)\n",
    "    gain = delta.where(delta > 0, 0.0)\n",
    "    loss = -delta.where(delta < 0, 0.0)\n",
    "    avg_gain = gain.rolling(window=period, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=period, min_periods=1).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def macd(series, fastperiod=12, slowperiod=26, signalperiod=9):\n",
    "    ema_fast = ema(series, fastperiod)\n",
    "    ema_slow = ema(series, slowperiod)\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = ema(macd_line, signalperiod)\n",
    "    return macd_line, signal_line\n",
    "\n",
    "def calculate_indicators(data):\n",
    "    data['RSI'] = rsi(data['Close'], period=14)\n",
    "    data['EMA_Fast'] = ema(data['Close'], period=3)\n",
    "    data['EMA_Slow'] = ema(data['Close'], period=9)\n",
    "    data['MACD'], data['MACD_Signal'] = macd(data['Close'])\n",
    "    envelope_length = 21\n",
    "    envelope_percent = 0.3 / 100\n",
    "    data['Envelope_Upper'] = ema(data['Close'], period=envelope_length) * (1 + envelope_percent)\n",
    "    data['Envelope_Lower'] = ema(data['Close'], period=envelope_length) * (1 - envelope_percent)\n",
    "    return data\n",
    "\n",
    "def calculate_scores(data):\n",
    "    data = data.reset_index(drop=True)\n",
    "    data['score_ema'] = 0\n",
    "    data['score_macd'] = 0\n",
    "    data['score_rsi30'] = 0\n",
    "    data['score_envelope'] = 0\n",
    "    data['sell_score_ema'] = 0\n",
    "    data['sell_score_macd'] = 0\n",
    "    data['sell_score_rsi70'] = 0\n",
    "    data['sell_score_envelope'] = 0\n",
    "    max_buy_score = 400  # Adjusted because there are 4 buy score components\n",
    "    max_sell_score = 400  # Adjusted because there are 4 sell score components\n",
    "\n",
    "    for i in range(1, len(data)):\n",
    "        # Buy scores\n",
    "        if (data['EMA_Fast'].iloc[i] > data['EMA_Slow'].iloc[i] and \n",
    "            data['EMA_Fast'].iloc[i-1] <= data['EMA_Slow'].iloc[i-1]):\n",
    "            data.at[i, 'score_ema'] = 100\n",
    "        else:\n",
    "            data.at[i, 'score_ema'] = max(0, data.iloc[i-1]['score_ema'] - 10)\n",
    "        \n",
    "        if (data['MACD'].iloc[i] > data['MACD_Signal'].iloc[i] and \n",
    "            data['MACD'].iloc[i] < 0 and \n",
    "            data['MACD'].iloc[i-1] <= data['MACD_Signal'].iloc[i-1]):\n",
    "            data.at[i, 'score_macd'] = 100\n",
    "        else:\n",
    "            data.at[i, 'score_macd'] = max(0, data.iloc[i-1]['score_macd'] - 10)\n",
    "        \n",
    "        if (data['RSI'].iloc[i] > 30 and \n",
    "            data['RSI'].iloc[i-1] <= 30):\n",
    "            data.at[i, 'score_rsi30'] = 100\n",
    "        else:\n",
    "            data.at[i, 'score_rsi30'] = max(0, data.iloc[i-1]['score_rsi30'] - 10)\n",
    "        \n",
    "        if (data['EMA_Fast'].iloc[i] > data['Envelope_Lower'].iloc[i] and \n",
    "            data['EMA_Fast'].iloc[i-1] <= data['Envelope_Lower'].iloc[i-1]):\n",
    "            data.at[i, 'score_envelope'] = 100\n",
    "        else:\n",
    "            data.at[i, 'score_envelope'] = max(0, data.iloc[i-1]['score_envelope'] - 10)\n",
    "        \n",
    "        # Sell scores\n",
    "        if (data['EMA_Fast'].iloc[i] < data['EMA_Slow'].iloc[i] and \n",
    "            data['EMA_Fast'].iloc[i-1] >= data['EMA_Slow'].iloc[i-1]):\n",
    "            data.at[i, 'sell_score_ema'] = 100\n",
    "        else:\n",
    "            data.at[i, 'sell_score_ema'] = max(0, data.iloc[i-1]['sell_score_ema'] - 10)\n",
    "        \n",
    "        if (data['MACD'].iloc[i] < data['MACD_Signal'].iloc[i] and \n",
    "            data['MACD'].iloc[i] > 0 and \n",
    "            data['MACD'].iloc[i-1] >= data['MACD_Signal'].iloc[i-1]):\n",
    "            data.at[i, 'sell_score_macd'] = 100\n",
    "        else:\n",
    "            data.at[i, 'sell_score_macd'] = max(0, data.iloc[i-1]['sell_score_macd'] - 10)\n",
    "        \n",
    "        if (data['RSI'].iloc[i] < 70 and \n",
    "            data['RSI'].iloc[i-1] >= 70):\n",
    "            data.at[i, 'sell_score_rsi70'] = 100\n",
    "        else:\n",
    "            data.at[i, 'sell_score_rsi70'] = max(0, data.iloc[i-1]['sell_score_rsi70'] - 10)\n",
    "        \n",
    "        if (data['EMA_Fast'].iloc[i] < data['Envelope_Upper'].iloc[i] and \n",
    "            data['EMA_Fast'].iloc[i-1] >= data['Envelope_Upper'].iloc[i-1]):\n",
    "            data.at[i, 'sell_score_envelope'] = 100\n",
    "        else:\n",
    "            data.at[i, 'sell_score_envelope'] = max(0, data.iloc[i-1]['sell_score_envelope'] - 10)\n",
    "        \n",
    "    data['score'] = data['score_ema'] + data['score_macd'] + data['score_rsi30'] + data['score_envelope']\n",
    "    data['sell_score'] = data['sell_score_ema'] + data['sell_score_macd'] + data['sell_score_rsi70'] + data['sell_score_envelope']\n",
    "    data['total_percent'] = (data['score'] / max_buy_score) * 100\n",
    "    data['total_sell_percent'] = (data['sell_score'] / max_sell_score) * 100\n",
    "\n",
    "    return data\n",
    "\n",
    "# Trading Environment\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data_full, data_obs, data_unscaled, scaler, transaction_cost=0.0001, max_stock=100, save_dir='frames',\n",
    "                 technical_indicators=True, technical_indicators_list=None, window_size=14):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data_full = data_full.reset_index(drop=True)  # Scaled data\n",
    "        self.data_unscaled = data_unscaled.reset_index(drop=True)  # Unscaled data for plotting and indicators\n",
    "        self.data = data_obs        # For observations (partially scaled)\n",
    "        self.current_step = 0\n",
    "        self.balance = 10000\n",
    "        self.initial_balance = 10000\n",
    "        self.stock_held = 0\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.max_stock = max_stock\n",
    "        self.window_size = window_size\n",
    "        self.save_dir = save_dir\n",
    "        self.technical_indicators = technical_indicators\n",
    "        self.technical_indicators_list = technical_indicators_list or []\n",
    "        self.cost_basis = 0.0\n",
    "        self.trade_history = []\n",
    "        self.position_returns = []\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Calculate custom indicators and scores on unscaled data\n",
    "        self.data_unscaled = calculate_indicators(self.data_unscaled)\n",
    "        self.data_unscaled = calculate_scores(self.data_unscaled)\n",
    "\n",
    "        # Update technical indicators list with all indicators and scores\n",
    "        self.technical_indicators_list = ['RSI', 'MACD', 'MACD_Signal', 'score', 'sell_score', 'total_percent', 'total_sell_percent']\n",
    "        self.tech_ind_columns = self.technical_indicators_list\n",
    "\n",
    "        if self.technical_indicators:\n",
    "            # Use the indicators from unscaled data for observations\n",
    "            self.tech_ind_data = self.data_unscaled[self.tech_ind_columns].fillna(0).values\n",
    "            num_tech_indicators = self.tech_ind_data.shape[1]\n",
    "            obs_shape = self.data.shape[1] + num_tech_indicators + 2  # Data columns + tech indicators + balance + stock_held\n",
    "            # Create a mapping from indicator names to indices\n",
    "            self.tech_ind_col_indices = {col: idx for idx, col in enumerate(self.tech_ind_columns)}\n",
    "        else:\n",
    "            obs_shape = self.data.shape[1] + 2  # Data columns + balance + stock_held\n",
    "\n",
    "        obs_low = -np.inf * np.ones(obs_shape)\n",
    "        obs_high = np.inf * np.ones(obs_shape)\n",
    "        self.observation_space = spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
    "\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.makedirs(self.save_dir)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        step = min(self.current_step, len(self.data) - 1)\n",
    "        obs = self.data[step]\n",
    "        if self.technical_indicators:\n",
    "            tech_ind_part = self.tech_ind_data[step]\n",
    "            obs = np.concatenate([\n",
    "                obs,\n",
    "                tech_ind_part,\n",
    "                [self.balance / self.initial_balance, self.stock_held / self.max_stock]\n",
    "            ]).astype(np.float32)\n",
    "        else:\n",
    "            obs = np.concatenate([\n",
    "                obs,\n",
    "                [self.balance / self.initial_balance, self.stock_held / self.max_stock]\n",
    "            ]).astype(np.float32)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.data_unscaled.iloc[self.current_step]['Close']  # Use unscaled price\n",
    "        current_date = self.data_unscaled.iloc[self.current_step]['timestamp']\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Execute the action\n",
    "        if action == 2:  # Buy\n",
    "            max_buyable = int(self.balance / (current_price * (1 + self.transaction_cost)))\n",
    "            buy_amount = min(max_buyable, self.max_stock - self.stock_held)\n",
    "            if buy_amount > 0:\n",
    "                total_cost = buy_amount * current_price * (1 + self.transaction_cost)\n",
    "                self.balance -= total_cost\n",
    "                self.stock_held += buy_amount\n",
    "                self.cost_basis = ((self.cost_basis * (self.stock_held - buy_amount)) + (current_price * buy_amount)) / self.stock_held\n",
    "                # Record trade\n",
    "                self.trade_history.append({\n",
    "                    'step': self.current_step,\n",
    "                    'type': 'buy',\n",
    "                    'amount': buy_amount,\n",
    "                    'price': current_price,\n",
    "                    'date': current_date\n",
    "                })\n",
    "        elif action == 0:  # Sell\n",
    "            if self.stock_held > 0:\n",
    "                sell_amount = self.stock_held\n",
    "                total_sale = sell_amount * current_price * (1 - self.transaction_cost)\n",
    "                self.balance += total_sale\n",
    "                position_return = (total_sale - (sell_amount * self.cost_basis)) / (sell_amount * self.cost_basis)\n",
    "                self.position_returns.append(position_return)\n",
    "                self.stock_held = 0\n",
    "                self.cost_basis = 0\n",
    "                # Record trade\n",
    "                self.trade_history.append({\n",
    "                    'step': self.current_step,\n",
    "                    'type': 'sell',\n",
    "                    'amount': sell_amount,\n",
    "                    'price': current_price,\n",
    "                    'date': current_date\n",
    "                })\n",
    "\n",
    "        # Get current scores\n",
    "        total_percent = self.data_unscaled.iloc[self.current_step]['total_percent']\n",
    "        total_sell_percent = self.data_unscaled.iloc[self.current_step]['total_sell_percent']\n",
    "\n",
    "        # Adjust reward based on your criteria\n",
    "        buy_threshold = 50\n",
    "        sell_threshold = 50\n",
    "\n",
    "        if action == 2:  # Buy\n",
    "            if total_percent >= buy_threshold:\n",
    "                reward += 1  # Reward for buying when score is high\n",
    "            else:\n",
    "                reward -= 1  # Penalty for buying when score is low\n",
    "        elif action == 0:  # Sell\n",
    "            if total_sell_percent >= sell_threshold:\n",
    "                reward += 1  # Reward for selling when sell score is high\n",
    "            else:\n",
    "                reward -= 1  # Penalty for selling when sell score is low\n",
    "\n",
    "        # Update current step\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.data):\n",
    "            done = True\n",
    "\n",
    "        # Calculate portfolio value for potential additional reward\n",
    "        portfolio_value = self.balance + self.stock_held * current_price\n",
    "        reward += (portfolio_value - self.initial_balance) / self.initial_balance\n",
    "\n",
    "        obs = self._get_observation()\n",
    "        info = {}\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.stock_held = 0\n",
    "        self.cost_basis = 0\n",
    "        self.trade_history = []\n",
    "        self.position_returns = []\n",
    "        obs = self._get_observation()\n",
    "        return obs\n",
    "\n",
    "    def render(self):\n",
    "        pass  # Optional: Implement rendering if needed\n",
    "\n",
    "def evaluate_model(model, env, num_episodes=1, dataset_name=''):\n",
    "    total_rewards = []\n",
    "    cumulative_rewards_list = []\n",
    "    position_returns = []\n",
    "\n",
    "    # Collect data for plotting\n",
    "    prices = []\n",
    "    actions = []\n",
    "    dates = []\n",
    "    positions = []\n",
    "    balances = []\n",
    "    stock_held_list = []\n",
    "    indicators = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        cumulative_rewards = []\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            cumulative_rewards.append(total_reward)\n",
    "\n",
    "            # Collect data\n",
    "            current_step = env.current_step\n",
    "            if current_step >= len(env.data_unscaled):\n",
    "                break\n",
    "            current_price = env.data_unscaled.iloc[current_step]['Close']  # Unscaled price\n",
    "            current_date = env.data_unscaled.iloc[current_step]['timestamp']\n",
    "            prices.append(current_price)\n",
    "            actions.append(action)\n",
    "            dates.append(current_date)\n",
    "            positions.append(env.stock_held)\n",
    "            balances.append(env.balance)\n",
    "            stock_held_list.append(env.stock_held)\n",
    "\n",
    "            # If indicators are available\n",
    "            if env.technical_indicators:\n",
    "                # Get indicators from unscaled data\n",
    "                current_indicators = env.data_unscaled.iloc[current_step][env.tech_ind_columns].values\n",
    "                indicators.append(current_indicators)\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        cumulative_rewards_list.append(cumulative_rewards)\n",
    "        position_returns.extend(env.position_returns)\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    win_rate = len([r for r in position_returns if r > 0]) / len(position_returns) if position_returns else 0\n",
    "    sharpe_ratio = np.mean(position_returns) / np.std(position_returns) if position_returns else 0\n",
    "\n",
    "    # Prepare results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Price': prices,\n",
    "        'Action': actions,\n",
    "        'Position': positions,\n",
    "        'Balance': balances,\n",
    "        'Stock Held': stock_held_list\n",
    "    })\n",
    "\n",
    "    # Add indicators if available\n",
    "    if env.technical_indicators:\n",
    "        indicators_df = pd.DataFrame(indicators, columns=env.tech_ind_columns)\n",
    "        results_df = pd.concat([results_df.reset_index(drop=True), indicators_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Calculate Portfolio Value\n",
    "    results_df['Portfolio Value'] = results_df['Balance'] + results_df['Stock Held'] * results_df['Price']\n",
    "\n",
    "    # Calculate Returns\n",
    "    results_df['Returns'] = results_df['Portfolio Value'].pct_change().fillna(0)\n",
    "\n",
    "    # Calculate Cumulative Returns\n",
    "    results_df['Cumulative Return'] = (1 + results_df['Returns']).cumprod() - 1\n",
    "\n",
    "    return avg_reward, win_rate, cumulative_rewards_list, sharpe_ratio, position_returns, results_df\n",
    "\n",
    "def walk_forward_split(data, train_months=2, val_months=1, test_months=1, step_months=1, start_date=None, end_date=None):\n",
    "    data = data.copy()\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], utc=True)\n",
    "    data = data.sort_values('timestamp')\n",
    "\n",
    "    # Apply start_date and end_date filters if provided\n",
    "    if start_date is not None:\n",
    "        start_datetime = pd.to_datetime(start_date).tz_localize('UTC')\n",
    "        data = data[data['timestamp'] >= start_datetime]\n",
    "    if end_date is not None:\n",
    "        end_datetime = pd.to_datetime(end_date).tz_localize('UTC')\n",
    "        data = data[data['timestamp'] <= end_datetime]\n",
    "\n",
    "    if data.empty:\n",
    "        print(\"No data available in the specified date range.\")\n",
    "        return []\n",
    "\n",
    "    min_available_date = data['timestamp'].min()\n",
    "    max_available_date = data['timestamp'].max()\n",
    "    \n",
    "    # Set the start_date and final_end_date based on available data\n",
    "    start_date = max(min_available_date, data['timestamp'].min())\n",
    "    final_end_date = min(max_available_date, data['timestamp'].max())\n",
    "\n",
    "    splits = []\n",
    "    current_start = start_date\n",
    "    # max_iterations = 1000\n",
    "    # iterations = 0\n",
    "\n",
    "    # while True:\n",
    "    #     iterations += 1\n",
    "    #     if iterations > max_iterations:\n",
    "    #         print(\"Maximum iterations reached. Breaking the loop to prevent infinite execution.\")\n",
    "    #         break\n",
    "\n",
    "    train_start = current_start\n",
    "    train_end = train_start + pd.DateOffset(months=train_months) - pd.DateOffset(seconds=1)\n",
    "    val_start = train_end + pd.DateOffset(seconds=1)\n",
    "    val_end = val_start + pd.DateOffset(months=val_months) - pd.DateOffset(seconds=1)\n",
    "    test_start = val_end + pd.DateOffset(seconds=1)\n",
    "    test_end = test_start + pd.DateOffset(months=test_months) - pd.DateOffset(seconds=1)\n",
    "\n",
    "    # if train_start > final_end_date:\n",
    "    #     print(\"Train start date exceeds final end date. Exiting loop.\")\n",
    "    #     break\n",
    "\n",
    "    # Adjust test_end if it exceeds the final available date\n",
    "    if test_end > final_end_date:\n",
    "        test_end = final_end_date\n",
    "\n",
    "    train_data = data[(data['timestamp'] >= train_start) & (data['timestamp'] <= train_end)]\n",
    "    val_data = data[(data['timestamp'] >= val_start) & (data['timestamp'] <= val_end)]\n",
    "    test_data = data[(data['timestamp'] >= test_start) & (data['timestamp'] <= test_end)]\n",
    "\n",
    "    # Ensure that each split has data\n",
    "    # if train_data.empty or val_data.empty or test_data.empty:\n",
    "    #     print(f\"No data for the period starting at {current_start}. Advancing current_start.\")\n",
    "    #     current_start += pd.DateOffset(months=step_months)\n",
    "    #     continue\n",
    "\n",
    "    split_info = {\n",
    "        'train_data': train_data,\n",
    "        'val_data': val_data,\n",
    "        'test_data': test_data,\n",
    "        'train_start_date': train_start,\n",
    "        'train_end_date': train_end,\n",
    "        'val_start_date': val_start,\n",
    "        'val_end_date': val_end,\n",
    "        'test_start_date': test_start,\n",
    "        'test_end_date': test_end\n",
    "    }\n",
    "\n",
    "    splits.append(split_info)\n",
    "    current_start += pd.DateOffset(months=step_months)\n",
    "\n",
    "    return splits\n",
    "\n",
    "def infer_new_data(new_data, model_filename, scaler_filename, technical_indicators_list, algo_class):\n",
    "    # Load the scaler\n",
    "    scaler = joblib.load(scaler_filename)\n",
    "\n",
    "    # Preprocess new data\n",
    "    new_data_full, new_data_obs, new_unscaled_data, _ = preprocess_data(\n",
    "        data=new_data,\n",
    "        scaler=scaler,\n",
    "        fit_scaler=False,\n",
    "        month_train=None  # No need to save scaler again\n",
    "    )\n",
    "\n",
    "    # Create environment\n",
    "    env = TradingEnv(\n",
    "        data_full=new_data_full,\n",
    "        data_obs=new_data_obs,\n",
    "        data_unscaled=new_unscaled_data,\n",
    "        scaler=scaler,\n",
    "        technical_indicators=True,\n",
    "        technical_indicators_list=technical_indicators_list,\n",
    "        window_size=14\n",
    "    )\n",
    "\n",
    "    # Load the model\n",
    "    model = algo_class.load(\n",
    "        model_filename,\n",
    "        env=env,\n",
    "        device='cuda',  # Change to 'cpu' if you don't have a GPU\n",
    "        custom_objects={\n",
    "            'features_extractor_class': CustomFeatureExtractor\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Run inference\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # Collect results\n",
    "    results = {\n",
    "        'dates': [],\n",
    "        'prices': [],\n",
    "        'actions': [],\n",
    "        'balances': [],\n",
    "        'positions': [],\n",
    "        'stock_held': [],\n",
    "        'indicators': []\n",
    "    }\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # Collect data\n",
    "        current_step = env.current_step\n",
    "        if current_step >= len(env.data_unscaled):\n",
    "            break\n",
    "        current_price = env.data_unscaled.iloc[current_step]['Close']  # Unscaled price\n",
    "        current_date = env.data_unscaled.iloc[current_step]['timestamp']\n",
    "        results['dates'].append(current_date)\n",
    "        results['prices'].append(current_price)\n",
    "        results['actions'].append(action)\n",
    "        results['balances'].append(env.balance)\n",
    "        results['positions'].append(env.stock_held)\n",
    "        results['stock_held'].append(env.stock_held)\n",
    "        if env.technical_indicators:\n",
    "            current_indicators = env.data_unscaled.iloc[current_step][env.tech_ind_columns].values\n",
    "            results['indicators'].append(current_indicators)\n",
    "\n",
    "    # Create a DataFrame with results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Date': results['dates'],\n",
    "        'Price': results['prices'],\n",
    "        'Action': results['actions'],\n",
    "        'Position': results['positions'],\n",
    "        'Balance': results['balances'],\n",
    "        'Stock Held': results['stock_held']\n",
    "    })\n",
    "\n",
    "    if env.technical_indicators:\n",
    "        indicators_df = pd.DataFrame(results['indicators'], columns=env.tech_ind_columns)\n",
    "        results_df = pd.concat([results_df.reset_index(drop=True), indicators_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Calculate Portfolio Value\n",
    "    results_df['Portfolio Value'] = results_df['Balance'] + results_df['Stock Held'] * results_df['Price']\n",
    "\n",
    "    # Return the results DataFrame\n",
    "    return results_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    filepath = 'test_06_08.csv'\n",
    "\n",
    "    # Load full data and fit scaler with the first training set\n",
    "    full_data, obs_data, unscaled_data, _ = preprocess_data(filepath, fit_scaler=False)\n",
    "\n",
    "    # Ensure 'timestamp' is in the dataframe\n",
    "    if 'timestamp' not in full_data.columns:\n",
    "        full_data['timestamp'] = unscaled_data['timestamp']\n",
    "\n",
    "    # Adjust your splits accordingly\n",
    "    # Adjust your splits accordingly\n",
    "    splits = walk_forward_split(\n",
    "        unscaled_data,\n",
    "        train_months=1,\n",
    "        val_months=1,\n",
    "        test_months=1,\n",
    "        step_months=0,\n",
    "        start_date='2024-06-01',\n",
    "        end_date='2024-08-30'\n",
    "    )\n",
    "\n",
    "    if not splits:\n",
    "        print(\"No splits were generated. Please check the date range and data availability.\")\n",
    "    else:\n",
    "        algorithms = {\n",
    "            'PPO': PPO,\n",
    "            'DQN': DQN\n",
    "        }\n",
    "\n",
    "        results_list = []\n",
    "\n",
    "        technical_indicators_list = ['RSI', 'MACD', 'MACD_Signal', 'score', 'sell_score', 'total_percent', 'total_sell_percent']\n",
    "\n",
    "        for split_index, split_data in enumerate(splits):\n",
    "            train_data = split_data['train_data']\n",
    "            val_data = split_data['val_data']\n",
    "            test_data = split_data['test_data']\n",
    "            train_start_date = split_data['train_start_date']\n",
    "            train_end_date = split_data['train_end_date']\n",
    "            val_start_date = split_data['val_start_date']\n",
    "            val_end_date = split_data['val_end_date']\n",
    "            test_start_date = split_data['test_start_date']\n",
    "            test_end_date = split_data['test_end_date']\n",
    "\n",
    "            # Get month_train from train_start_date\n",
    "            month_train = train_start_date.strftime('%Y%m')\n",
    "\n",
    "            # Detect price patterns\n",
    "            train_pattern = detect_price_pattern(train_data['Close'].values)\n",
    "            test_pattern = detect_price_pattern(test_data['Close'].values)\n",
    "\n",
    "            print(f\"\\nWalk-Forward Split {split_index + 1}\")\n",
    "            print(f\"Train Data Range: {train_start_date.date()} to {train_end_date.date()} - Pattern: {train_pattern}\")\n",
    "            print(f\"Validation Data Range: {val_start_date.date()} to {val_end_date.date()}\")\n",
    "            print(f\"Test Data Range: {test_start_date.date()} to {test_end_date.date()} - Pattern: {test_pattern}\")\n",
    "\n",
    "            for algo_name, algo_class in algorithms.items():\n",
    "                print(f\"\\nTraining {algo_name} on Split {split_index + 1} with Technical Indicators...\")\n",
    "\n",
    "                model = None\n",
    "                policy_kwargs = dict(\n",
    "                    features_extractor_class=CustomFeatureExtractor,\n",
    "                    net_arch=[256, 256],\n",
    "                    activation_fn=th.nn.ReLU\n",
    "                )\n",
    "\n",
    "                # Preprocess training data and save scaler with month_train\n",
    "                train_data_full, train_data_obs, train_unscaled, scaler = preprocess_data(\n",
    "                    data=train_data,\n",
    "                    scaler=None,\n",
    "                    fit_scaler=True,\n",
    "                    month_train=month_train\n",
    "                )\n",
    "\n",
    "                # Preprocess validation data\n",
    "                val_data_full, val_data_obs, val_unscaled, _ = preprocess_data(\n",
    "                    data=val_data,\n",
    "                    scaler=scaler,\n",
    "                    fit_scaler=False,\n",
    "                    month_train=month_train\n",
    "                )\n",
    "\n",
    "                env_train = TradingEnv(\n",
    "                    data_full=train_data_full,\n",
    "                    data_obs=train_data_obs,\n",
    "                    data_unscaled=train_unscaled,\n",
    "                    scaler=scaler,\n",
    "                    technical_indicators=True,\n",
    "                    technical_indicators_list=technical_indicators_list,\n",
    "                    window_size=14\n",
    "                )\n",
    "\n",
    "                env_val = TradingEnv(\n",
    "                    data_full=val_data_full,\n",
    "                    data_obs=val_data_obs,\n",
    "                    data_unscaled=val_unscaled,\n",
    "                    scaler=scaler,\n",
    "                    technical_indicators=True,\n",
    "                    technical_indicators_list=technical_indicators_list,\n",
    "                    window_size=14\n",
    "                )\n",
    "\n",
    "                model = algo_class(\n",
    "                    'MlpPolicy',\n",
    "                    env_train,\n",
    "                    policy_kwargs=policy_kwargs,\n",
    "                    verbose=1,\n",
    "                    device='cuda'  # Change to 'cpu' if you don't have a GPU\n",
    "                )\n",
    "\n",
    "                eval_callback = EvalCallback(\n",
    "                    env_val,\n",
    "                    best_model_save_path=f'./logs/{algo_name}_split{split_index + 1}/',\n",
    "                    log_path=f'./logs/{algo_name}_split{split_index + 1}/',\n",
    "                    eval_freq=10000,\n",
    "                    deterministic=True,\n",
    "                    render=False\n",
    "                )\n",
    "\n",
    "                model.learn(total_timesteps=100000, callback=eval_callback)\n",
    "\n",
    "                best_model_path = f'./logs/{algo_name}_split{split_index + 1}/best_model.zip'\n",
    "                if os.path.exists(best_model_path):\n",
    "                    model = algo_class.load(\n",
    "                        best_model_path,\n",
    "                        env=env_train,\n",
    "                        device='cuda',  # Change to 'cpu' if you don't have a GPU\n",
    "                        custom_objects={\n",
    "                            'features_extractor_class': CustomFeatureExtractor\n",
    "                        }\n",
    "                    )\n",
    "                    print(f\"Loaded best model for {algo_name} from validation.\")\n",
    "                else:\n",
    "                    print(f\"No best model found for {algo_name}; using last trained model.\")\n",
    "\n",
    "                model_filename = f\"{algo_name}_Split{split_index + 1}_{month_train}.zip\"\n",
    "                model.save(model_filename)\n",
    "                print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "                avg_reward_train, win_rate_train, cumulative_rewards_train, sharpe_ratio_train, position_returns_train, train_results_df = evaluate_model(model, env_train, dataset_name='Train')\n",
    "                avg_reward_val, win_rate_val, cumulative_rewards_val, sharpe_ratio_val, position_returns_val, val_results_df = evaluate_model(model, env_val, dataset_name='Validation')\n",
    "\n",
    "                # Prepare test environment\n",
    "                test_data_full, test_data_obs, test_unscaled, _ = preprocess_data(\n",
    "                    data=test_data,\n",
    "                    scaler=scaler,\n",
    "                    fit_scaler=False,\n",
    "                    month_train=month_train\n",
    "                )\n",
    "\n",
    "                env_test = TradingEnv(\n",
    "                    data_full=test_data_full,\n",
    "                    data_obs=test_data_obs,\n",
    "                    data_unscaled=test_unscaled,\n",
    "                    scaler=scaler,\n",
    "                    technical_indicators=True,\n",
    "                    technical_indicators_list=technical_indicators_list,\n",
    "                    window_size=14\n",
    "                )\n",
    "\n",
    "                loaded_model = algo_class.load(\n",
    "                    model_filename,\n",
    "                    env=env_test,\n",
    "                    device='cuda',  # Change to 'cpu' if you don't have a GPU\n",
    "                    custom_objects={\n",
    "                        'features_extractor_class': CustomFeatureExtractor\n",
    "                    }\n",
    "                )\n",
    "                print(f\"Model loaded from {model_filename}\")\n",
    "\n",
    "                avg_reward_test, win_rate_test, cumulative_rewards_test, sharpe_ratio_test, position_returns_test, test_results_df = evaluate_model(loaded_model, env_test, dataset_name='Test')\n",
    "\n",
    "                # Save results dataframes for interpretation\n",
    "                train_results_df.to_csv(f'{algo_name}_Split{split_index + 1}_Train_Results.csv', index=False)\n",
    "                val_results_df.to_csv(f'{algo_name}_Split{split_index + 1}_Validation_Results.csv', index=False)\n",
    "                test_results_df.to_csv(f'{algo_name}_Split{split_index + 1}_Test_Results.csv', index=False)\n",
    "\n",
    "                results_list.extend([\n",
    "                    {\n",
    "                        'Algorithm': algo_name,\n",
    "                        'Split': split_index + 1,\n",
    "                        'Dataset': 'Train',\n",
    "                        'Start Date': train_start_date.date(),\n",
    "                        'End Date': train_end_date.date(),\n",
    "                        'Price Pattern': train_pattern,\n",
    "                        'Avg Reward': avg_reward_train,\n",
    "                        'Win Rate': win_rate_train,\n",
    "                        'Sharpe Ratio': sharpe_ratio_train,\n",
    "                        'Avg Position Return': np.mean(position_returns_train)\n",
    "                    },\n",
    "                    {\n",
    "                        'Algorithm': algo_name,\n",
    "                        'Split': split_index + 1,\n",
    "                        'Dataset': 'Validation',\n",
    "                        'Start Date': val_start_date.date(),\n",
    "                        'End Date': val_end_date.date(),\n",
    "                        'Avg Reward': avg_reward_val,\n",
    "                        'Win Rate': win_rate_val,\n",
    "                        'Sharpe Ratio': sharpe_ratio_val,\n",
    "                        'Avg Position Return': np.mean(position_returns_val)\n",
    "                    },\n",
    "                    {\n",
    "                        'Algorithm': algo_name,\n",
    "                        'Split': split_index + 1,\n",
    "                        'Dataset': 'Test',\n",
    "                        'Start Date': test_start_date.date(),\n",
    "                        'End Date': test_end_date.date(),\n",
    "                        'Price Pattern': test_pattern,\n",
    "                        'Avg Reward': avg_reward_test,\n",
    "                        'Win Rate': win_rate_test,\n",
    "                        'Sharpe Ratio': sharpe_ratio_test,\n",
    "                        'Avg Position Return': np.mean(position_returns_test)\n",
    "                    }\n",
    "                ])\n",
    "\n",
    "        results_df = pd.DataFrame(results_list)\n",
    "        print(\"\\nDetailed Results:\")\n",
    "        print(results_df)\n",
    "\n",
    "        results_df.to_csv('trading_rl_results_with_custom_criteria.csv', index=False)\n",
    "\n",
    "        # Example of using the inference function with new data\n",
    "        # Assuming you have new data in 'data/new_data.csv'\n",
    "        # new_data = pd.read_csv('data/new_data.csv')\n",
    "        # results_df = infer_new_data(\n",
    "        #     new_data=new_data,\n",
    "        #     model_filename=model_filename,\n",
    "        #     scaler_filename=f'scaler_{month_train}.save',\n",
    "        #     technical_indicators_list=technical_indicators_list,\n",
    "        #     algo_class=algorithms[algo_name]\n",
    "        # )\n",
    "        # print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
